{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install langchain-together\n",
    "# %pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asha4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from langchain_together import Together\n",
    "import time\n",
    "import os\n",
    "from fpdf import FPDF\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_with_sprecher(df, sprecher_prefix):\n",
    "    df = df.dropna(subset=['Sprecher'])\n",
    "    filtered_rows = df[df['Sprecher'].str.startswith(sprecher_prefix)]\n",
    "    transkript_list = filtered_rows['Transkript'].tolist()\n",
    "    return transkript_list\n",
    "\n",
    "def transkript_to_string(transkript_list):\n",
    "    return \"\\n\".join(transkript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_chunks(text, max_words_per_chunk):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(word_tokenize(sentence))\n",
    "        if current_word_count + words_in_sentence > max_words_per_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_word_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += words_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.llm = Together(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            temperature=0.1,\n",
    "            max_tokens= 1024,\n",
    "            top_k=1,\n",
    "            together_api_key=\"3547c556949e8b9beea25d84c997eebd60a491f60e33548b59a51f86f313a277\"\n",
    "        )\n",
    "\n",
    "#160be45af627da8e0d12935e59de9db8648a6327ba2a81a563bacb39750a052e\n",
    "\n",
    "    def invoke_with_retry(self, full_input, retries=3, retry_delay=30):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                output_summary = self.llm.invoke(full_input)\n",
    "                return output_summary\n",
    "            except Exception as e:\n",
    "                if \"524\" in str(e) and attempt < retries - 1:\n",
    "                    print(f\"Error 524: Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retries += 1\n",
    "\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    def generate_biography(self, input_text):\n",
    "        self.llm.temperature = 0.1\n",
    "        self.llm.max_tokens = 1024\n",
    "        self.llm.top_k = 1\n",
    "        \n",
    "        prompt = \"\"\"\n",
    "        Du bist ein deutsches Textzusammenfassungsmodell. Erstellen Sie eine prägnante Zusammenfassung des obigen Textes in deutscher Sprache innerhalb von 500 Wörtern. Konzentrieren Sie sich auf die wichtigsten Punkte und bewahren Sie Klarheit. Work only with the data given and do not provide your conclusions or interpretations of the biography or the data provided. Work only with the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk = 70000)\n",
    "        biography_parts = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            full_input = chunk + prompt\n",
    "            output_summary = self.invoke_with_retry(full_input)\n",
    "            biography_parts.append(output_summary.strip())\n",
    "\n",
    "        full_biography = \" \".join(biography_parts)\n",
    "        return full_biography\n",
    "\n",
    "    def extend_biography(self, partial_biography, input_text):\n",
    "        self.llm.temperature = 0.1\n",
    "        self.llm.max_tokens = 800 \n",
    "        self.llm.top_k = 1\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Hier ist der erste Teil der Biografie: {partial_biography}\n",
    "        Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "        \"\"\"\n",
    "\n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=70000)\n",
    "        extended_biography_parts = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            full_input = chunk + prompt\n",
    "            output_summary = self.invoke_with_retry(full_input)\n",
    "            extended_biography_parts.append(output_summary.strip())\n",
    "\n",
    "        full_extended_biography = \" \".join(extended_biography_parts)\n",
    "        return self.remove_incomplete_sentence(full_extended_biography)\n",
    "\n",
    "    def refine_biography_to_500_words(self, extended_biography, input_text):\n",
    "        self.llm.temperature = 0.1\n",
    "        self.llm.max_tokens = 512\n",
    "        self.llm.top_k = 1\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Hier ist der erste Teil der Biografie: {extended_biography}\n",
    "        Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "                \"\"\"\n",
    "        \n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=70000)\n",
    "        refined_parts = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            full_input = chunk + prompt\n",
    "            output_summary = self.invoke_with_retry(full_input)\n",
    "            refined_parts.append(output_summary.strip())\n",
    "\n",
    "        full_refined_biography = \" \".join(refined_parts)\n",
    "        return self.remove_incomplete_sentence(full_refined_biography)\n",
    "\n",
    "    def remove_incomplete_sentence(self, biography):\n",
    "        words = word_tokenize(biography)\n",
    "        if len(words) <= 500:\n",
    "            return biography\n",
    "        \n",
    "        truncated_words = words[:500]\n",
    "        truncated_text = \" \".join(truncated_words)\n",
    "        last_full_stop_index = truncated_text.rfind('.')\n",
    "        \n",
    "        if last_full_stop_index != -1:\n",
    "            return truncated_text[:last_full_stop_index + 1]\n",
    "        else:\n",
    "            return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    summarizer = Summarizer()\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        df = read_csv(file_path)\n",
    "        if df is not None:\n",
    "            sprecher_prefix = 'IP_'\n",
    "            transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "            transcript_data = transkript_to_string(transkript_list)\n",
    "        else:\n",
    "            print(\"Failed to read CSV file.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Unsupported file format.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Step 1: Generate the initial biography with chunking\n",
    "#     initial_biography = summarizer.generate_biography(transcript_data)\n",
    "#     print(\"Initial Biography : \\n\", initial_biography.strip())\n",
    "\n",
    "#     # Step 2: Extend the biography with additional details, also using chunking\n",
    "#     extended_biography = summarizer.extend_biography(initial_biography, transcript_data)\n",
    "#     print(\"Extended Biography : \\n\", extended_biography.strip())\n",
    "\n",
    "#     # Step 3: Refine the biography to 500 words and remove incomplete sentences\n",
    "#     refined_biography = summarizer.refine_biography_to_500_words(extended_biography, transcript_data)\n",
    "#     print(\"Refined Biography (500 words) : \\n\", refined_biography.strip())\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     file_path = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/adg0001_er_2024_04_23.csv\"\n",
    "#     process_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported file format: ADG0001 Bio.txt\n",
      "Processed and saved adg0001_er_2024_04_23.csv as PDF.\n",
      "Unsupported file format: ADG0002 Bio.txt\n",
      "Processed and saved adg0002_er_2024_04_23.csv as PDF.\n",
      "Unsupported file format: Output\n",
      "Unsupported file format: output_pdfs\n"
     ]
    }
   ],
   "source": [
    "def save_text_to_pdf(text, pdf_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    # Add text to the PDF\n",
    "    for line in text.split('\\n'):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    \n",
    "    # Save the PDF\n",
    "    pdf.output(pdf_path)\n",
    "\n",
    "def process_all_files_in_directory(directory_path):\n",
    "    summarizer = Summarizer()\n",
    "    output_directory = os.path.join(directory_path, \"output_pdfs\")\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    # Iterate over all CSV files in the directory\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            df = read_csv(file_path)\n",
    "            \n",
    "            if df is not None:\n",
    "                sprecher_prefix = 'IP_'\n",
    "                transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "                transcript_data = transkript_to_string(transkript_list)\n",
    "                \n",
    "                # Step 1: Generate the initial biography with chunking\n",
    "                initial_biography = summarizer.generate_biography(transcript_data)\n",
    "                \n",
    "                # Step 2: Extend the biography with additional details, also using chunking\n",
    "                extended_biography = summarizer.extend_biography(initial_biography, transcript_data)\n",
    "                \n",
    "                # Step 3: Refine the biography to 500 words and remove incomplete sentences\n",
    "                refined_biography = summarizer.refine_biography_to_500_words(extended_biography, transcript_data)\n",
    "                \n",
    "                # Save the refined biography as a PDF\n",
    "                output_pdf_path = os.path.join(output_directory, file_name.replace('.csv', '.pdf'))\n",
    "                save_text_to_pdf(refined_biography.strip(), output_pdf_path)\n",
    "                print(f\"Processed and saved {file_name} as PDF.\")\n",
    "            else:\n",
    "                print(f\"Failed to read CSV file: {file_name}\")\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/\"\n",
    "    process_all_files_in_directory(directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
