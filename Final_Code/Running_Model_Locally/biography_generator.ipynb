{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the terminal after running the two blocks below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install fpdf\n",
    "# %pip install ipywidgets\n",
    "# %pip install transformers==4.43.1\n",
    "# %pip install vllm==0.5.3.post1\n",
    "# %pip install torch\n",
    "# %pip install IPython.display\n",
    "# %pip install gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=\"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\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\EDWIN\n",
      "[nltk_data]     SAMUEL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fpdf import FPDF\n",
    "from ipywidgets import FileUpload, Button\n",
    "from IPython.display import display\n",
    "from io import StringIO\n",
    "import torch\n",
    "import gc\n",
    "import nltk\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file):\n",
    "    try:\n",
    "        # Use BytesIO to read the file correctly\n",
    "        file.stream.seek(0)\n",
    "        file_content = file.read()  # Read file content\n",
    "        df = pd.read_csv(BytesIO(file_content), sep='\\t')  # Use BytesIO to pass the content\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_docx(file):\n",
    "    try:\n",
    "        file.stream.seek(0)\n",
    "        doc = Document(BytesIO(file.read()))\n",
    "        paragraphs = [p.text for p in doc.paragraphs if p.text.strip() != '']\n",
    "        return \"\\n\".join(paragraphs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_pdf(file):\n",
    "    try:\n",
    "        file.stream.seek(0)\n",
    "        reader = PdfReader(BytesIO(file.read()))\n",
    "        text = []\n",
    "        for page in reader.pages:\n",
    "            text.append(page.extract_text())\n",
    "        return \"\\n\".join(filter(None, text))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_with_sprecher(df, sprecher_prefix):\n",
    "    df = df.dropna(subset=['Sprecher'])\n",
    "    filtered_rows = df[df['Sprecher'].str.startswith(sprecher_prefix)]\n",
    "    transkript_list = filtered_rows['Transkript'].tolist()\n",
    "    return transkript_list\n",
    "\n",
    "def transkript_to_string(transkript_list):\n",
    "    return \"\\n\".join(transkript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_chunks(text, max_words_per_chunk):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(word_tokenize(sentence))\n",
    "        if current_word_count + words_in_sentence > max_words_per_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_word_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += words_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "#Use this if the above chunking generates a lot of load on memory\n",
    "\n",
    "# def divide_into_chunks(text, max_words_per_chunk):\n",
    "#     words = word_tokenize(text)\n",
    "#     return [' '.join(words[i:i + max_words_per_chunk]) for i in range(0, len(words), max_words_per_chunk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer, self.model = self._load_model()  # Load model once in initialization\n",
    "        self.temperature = 0.1\n",
    "        self.max_tokens = 1024\n",
    "        self.top_k = 1\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the tokenizer and model once during initialization.\"\"\"\n",
    "        gc.collect()  # Clear memory\n",
    "        print(\"Loading model...\")\n",
    "\n",
    "        # Specify the directory where the model will be downloaded\n",
    "        model_directory = \"E:/Meta-Llama-Model\"\n",
    "        \n",
    "        token = \"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\"\n",
    "\n",
    "        # Download the tokenizer and model, and save them to the specified directory\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "            use_auth_token=token,\n",
    "            cache_dir=model_directory  # Specify the download location\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \n",
    "            use_auth_token=token,\n",
    "            cache_dir=model_directory  # Specify the download location\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(f\"Model and tokenizer loaded successfully to {model_directory}.\")\n",
    "        return tokenizer, model\n",
    "\n",
    "    def _generate(self, text, prompt):\n",
    "        \"\"\"Generate text using the loaded model.\"\"\"\n",
    "        inputs = self.tokenizer(text + prompt, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],  # Add the attention mask\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def generate_biography(self, input_text):\n",
    "        \n",
    "        self.temperature = 0.1\n",
    "        self.max_tokens = 1024\n",
    "        self.top_k = 1\n",
    "        \n",
    "        prompt = \"\"\"\n",
    "\n",
    "I would like you to generate biography of an interviewee based on the following structured questions in German language and  written in third-person-view, make sure it is in german language only. Please address each question thoroughly, ensuring that the narrative flows smoothly from one life stage to the next. The biography should include the following information:\n",
    "\n",
    "Birth and Early Family Life:\n",
    "\n",
    "When and where was the interviewee born? Include the date and location of birth.\n",
    "Who are the interviewee's parents? Provide their names, backgrounds, and any relevant details about their lives.\n",
    "Does the interviewee have any siblings? If so, provide details about them, including names and relationships.\n",
    "Education:\n",
    "\n",
    "Which school or schools did the interviewee attend? Mention the date, names of the institutions, locations, and any significant experiences or achievements during their education.\n",
    "Career and Professional Life:\n",
    "\n",
    "What profession did the interviewee learn or train for? Mention date and describe the nature of their training or education in this field.\n",
    "Which jobs or professions has the interviewee practiced? Include details about the dates, roles, companies, or organizations they worked for, and any significant milestones or achievements in their career.\n",
    "Life Events and Personal Milestones:\n",
    "\n",
    "What were the formative or significant life events with years mentioned in the interviewee's childhood? Mention dates, Include any experiences that had a lasting impact.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adolescence?  Mention dates, Describe how these events influenced their path in life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's early adult years? Include details about any dates, transitions, challenges, or accomplishments during this period.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adult years?  Mention dates, Describe key experiences that shaped their personal or professional life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's late adult years? Highlight any dates, major changes, achievements, or reflections during this time.\n",
    "Personal Life:\n",
    "\n",
    "Did the interviewee marry  with years mentioned? If yes, provide details about their spouse,dates, including the name and any significant information about their relationship.\n",
    "Does the interviewee have children with years mentioned? If so, provide details about their children, dates, including names and any significant life events related to them.\n",
    "Significant Life Events:\n",
    "\n",
    "What are the most significant life events that have shaped the interviewee's life with years mentioned? Reflect on how these events with years mentioned impacted their personal growth, relationships, or career, dates.\n",
    "Please ensure the biography is coherent, chronological, detailed, and presents a well-rounded view of the interviewee's life journey with years mentioned. Include years, don't forget any years mentioned in the interview.\n",
    "   \n",
    "           \"\"\"\n",
    "        \n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=25000)\n",
    "        biography_parts = [self._generate(chunk, prompt) for chunk in chunks]\n",
    "        return \" \".join(biography_parts)\n",
    "\n",
    "\n",
    "\n",
    "    def extend_biography(self, partial_biography, input_text):\n",
    "        self.temperature = 0.1\n",
    "        self.max_tokens = 1024\n",
    "        self.top_k = 1\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "            Hier ist der erste Teil der Biografie: {partial_biography}\n",
    "I would like you to generate biography of an interviewee based on the following structured questions in German language and  written in third-person-view, make sure it is in german language only. Please address each question thoroughly, ensuring that the narrative flows smoothly from one life stage to the next. The biography should include the following information:\n",
    "\n",
    "Birth and Early Family Life:\n",
    "\n",
    "When and where was the interviewee born? Include the date and location of birth.\n",
    "Who are the interviewee's parents? Provide their names, backgrounds, and any relevant details about their lives.\n",
    "Does the interviewee have any siblings? If so, provide details about them, including names and relationships.\n",
    "Education:\n",
    "\n",
    "Which school or schools did the interviewee attend? Mention the date, names of the institutions, locations, and any significant experiences or achievements during their education.\n",
    "Career and Professional Life:\n",
    "\n",
    "What profession did the interviewee learn or train for? Mention date and describe the nature of their training or education in this field.\n",
    "Which jobs or professions has the interviewee practiced? Include details about the dates, roles, companies, or organizations they worked for, and any significant milestones or achievements in their career.\n",
    "Life Events and Personal Milestones:\n",
    "\n",
    "What were the formative or significant life events with years mentioned in the interviewee's childhood? Mention dates, Include any experiences that had a lasting impact.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adolescence?  Mention dates, Describe how these events influenced their path in life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's early adult years? Include details about any dates, transitions, challenges, or accomplishments during this period.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adult years?  Mention dates, Describe key experiences that shaped their personal or professional life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's late adult years? Highlight any dates, major changes, achievements, or reflections during this time.\n",
    "Personal Life:\n",
    "\n",
    "Did the interviewee marry  with years mentioned? If yes, provide details about their spouse,dates, including the name and any significant information about their relationship.\n",
    "Does the interviewee have children with years mentioned? If so, provide details about their children, dates, including names and any significant life events related to them.\n",
    "Significant Life Events:\n",
    "\n",
    "What are the most significant life events that have shaped the interviewee's life with years mentioned? Reflect on how these events with years mentioned impacted their personal growth, relationships, or career, dates.\n",
    "Please ensure the biography is coherent, chronological, detailed, and presents a well-rounded view of the interviewee's life journey with years mentioned. Include years, don't forget any years mentioned in the interview.\n",
    "   \n",
    "            \"\"\"\n",
    "\n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=25000)\n",
    "        biography_parts = [self._generate(chunk, prompt) for chunk in chunks]\n",
    "        return \" \".join(biography_parts)\n",
    "\n",
    "\n",
    "\n",
    "    def refine_biography_to_500_words(self, extended_biography, input_text):\n",
    "\n",
    "        self.temperature = 0.1\n",
    "        self.max_tokens = 1024\n",
    "        self.top_k = 1\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "            Hier ist der erste Teil der Biografie: {partial_biography}\n",
    "I would like you to generate biography of an interviewee based on the following structured questions in German language and  written in third-person-view, make sure it is in german language only. Please address each question thoroughly, ensuring that the narrative flows smoothly from one life stage to the next. The biography should include the following information:\n",
    "\n",
    "Birth and Early Family Life:\n",
    "\n",
    "When and where was the interviewee born? Include the date and location of birth.\n",
    "Who are the interviewee's parents? Provide their names, backgrounds, and any relevant details about their lives.\n",
    "Does the interviewee have any siblings? If so, provide details about them, including names and relationships.\n",
    "Education:\n",
    "\n",
    "Which school or schools did the interviewee attend? Mention the date, names of the institutions, locations, and any significant experiences or achievements during their education.\n",
    "Career and Professional Life:\n",
    "\n",
    "What profession did the interviewee learn or train for? Mention date and describe the nature of their training or education in this field.\n",
    "Which jobs or professions has the interviewee practiced? Include details about the dates, roles, companies, or organizations they worked for, and any significant milestones or achievements in their career.\n",
    "Life Events and Personal Milestones:\n",
    "\n",
    "What were the formative or significant life events with years mentioned in the interviewee's childhood? Mention dates, Include any experiences that had a lasting impact.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adolescence?  Mention dates, Describe how these events influenced their path in life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's early adult years? Include details about any dates, transitions, challenges, or accomplishments during this period.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adult years?  Mention dates, Describe key experiences that shaped their personal or professional life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's late adult years? Highlight any dates, major changes, achievements, or reflections during this time.\n",
    "Personal Life:\n",
    "\n",
    "Did the interviewee marry  with years mentioned? If yes, provide details about their spouse,dates, including the name and any significant information about their relationship.\n",
    "Does the interviewee have children with years mentioned? If so, provide details about their children, dates, including names and any significant life events related to them.\n",
    "Significant Life Events:\n",
    "\n",
    "What are the most significant life events that have shaped the interviewee's life with years mentioned? Reflect on how these events with years mentioned impacted their personal growth, relationships, or career, dates.\n",
    "Please ensure the biography is coherent, chronological, detailed, and presents a well-rounded view of the interviewee's life journey with years mentioned. Include years, don't forget any years mentioned in the interview.\n",
    "  \n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=25000)\n",
    "        biography_parts = [self._generate(chunk, prompt) for chunk in chunks]\n",
    "        return \" \".join(biography_parts)\n",
    "\n",
    "\n",
    "\n",
    "    def remove_incomplete_sentence(self, biography):\n",
    "        words = nltk.word_tokenize(biography)\n",
    "        if len(words) <= 800:\n",
    "            return biography\n",
    "        \n",
    "        truncated_words = words[:800]\n",
    "        truncated_text = \" \".join(truncated_words)\n",
    "        last_full_stop_index = truncated_text.rfind('.')\n",
    "        \n",
    "        if last_full_stop_index != -1:\n",
    "            return truncated_text[:last_full_stop_index + 1]\n",
    "        else:\n",
    "            return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the generated biography as a PDF\n",
    "def save_text_to_pdf(text, pdf_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    \n",
    "    pdf.output(pdf_path)\n",
    "    print(f\"Biography saved as {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# # Function to handle reading CSV, DOCX, or PDF\n",
    "# def process_file_and_update_status(file_path):\n",
    "#     file_name = os.path.basename(file_path)  # Extract only the file name\n",
    "#     try:\n",
    "#         transcript_data = None\n",
    "\n",
    "#         if file_name.endswith('.csv'):\n",
    "#             # Specify the correct delimiter as tab (\\t)\n",
    "#             df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "            \n",
    "#             # Print the column names to check what's in the CSV\n",
    "#             print(\"Available columns in the CSV: \", df.columns)\n",
    "\n",
    "#             if 'Transkript' in df.columns:\n",
    "#                 df['Transkript'] = df['Transkript'].fillna('').astype(str)\n",
    "#                 transcript_data = \"\\n\".join(df['Transkript'].tolist())\n",
    "#             else:\n",
    "#                 print(f\"Error: 'Transkript' column not found in the CSV.\")\n",
    "#                 return\n",
    "#         elif file_name.endswith('.docx'):\n",
    "#             transcript_data = read_docx(file_path)\n",
    "#         elif file_name.endswith('.pdf'):\n",
    "#             transcript_data = read_pdf(file_path)\n",
    "#         else:\n",
    "#             print(\"Unsupported file format.\")\n",
    "#             return\n",
    "\n",
    "#         if transcript_data:\n",
    "#             summarizer = Summarizer()\n",
    "#             biography = summarizer.generate_biography(transcript_data)\n",
    "#             extend_biography = summarizer.extend_biography(biography)\n",
    "#             final_biography = summarizer.remove_incomplete_sentence(extend_biography)\n",
    "\n",
    "#             output_pdf_path = os.path.join('processed', file_name.rsplit('.', 1)[0] + '.pdf')\n",
    "#             print(f\"PDF saved at: {os.path.abspath(output_pdf_path)}\")\n",
    "\n",
    "#             save_text_to_pdf(final_biography, output_pdf_path)\n",
    "\n",
    "#         else:\n",
    "#             print(\"No transcript data found. Status: Failed.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "# # Example usage\n",
    "# file_path = '/home/jovyan/adg0001_er_2024_04_23.csv'\n",
    "# process_file_and_update_status(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path in datalabs\n",
    "/home/jovyan/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the CSV:  Index(['Band', 'Timecode', 'Sprecher', 'Transkript', 'Übersetzung',\n",
      "       'Hauptüberschrift', 'Zwischenüberschrift',\n",
      "       'Hauptüberschrift (Übersetzung)', 'Zwischenüberschrift (Übersetzung)',\n",
      "       'Registerverknüpfungen', 'Anmerkungen', 'Anmerkungen (Übersetzung)'],\n",
      "      dtype='object')\n",
      "Transcript Data: Können wir anfangen?\n",
      "Also wäre schön, wenn Sie mit Kindheit beginnen würden.\n",
      "Ich war das erste Enkelkind, einzige Enkelkind, lange Zeit und bin sehr verwöhnt worden, da ich ziemlich viel bei der Großmutter gewesen bin.\n",
      "Ich bin gebürtig aus Hemer im Sauerland und bin 29.5.25 geboren.\n",
      "Und, ja, Kindheit verlief eigentlich an und für sich normal, habe allerdings ziemlich unter Migräne zu leiden gehabt schon als Vorschulkind, was sich während der Schulzeit verschlechterte und verschlimmerte.\n",
      "Ich hätt\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EDWIN SAMUEL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:785: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e8bc2b77f244fdb25037ac4f8d8914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EDWIN SAMUEL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\EDWIN SAMUEL\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3.1-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246b45170c7b451b8a3b610c3d467f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77385ef94a3845b4a543195a2393537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EDWIN SAMUEL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d226cef388457b864c88de9808509c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e9db5118db42aa8bd4e110a5c66073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0a4a18fd6e459fa3e9a5eb870e5ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c7acdb9e8145acbbc1f82a7e98d83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efda1617d2a42269c0b292cc8dd25ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EDWIN SAMUEL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 4915.92 MB. The target location C:\\Users\\EDWIN SAMUEL\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3.1-8B-Instruct\\blobs only has 3747.52 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a2186e6a934c4981bdb0fe10a3e93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file adg0001_er_2024_04_23.csv: [Errno 28] No space left on device\n"
     ]
    }
   ],
   "source": [
    "# Function to handle reading CSV, DOCX, or PDF\n",
    "def process_file_and_update_status(file_path):\n",
    "    file_name = os.path.basename(file_path)  # Extract only the file name\n",
    "    try:\n",
    "        transcript_data = None\n",
    "\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Specify the correct delimiter as tab (\\t)\n",
    "            df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "            \n",
    "            # Print the column names to check what's in the CSV\n",
    "            print(\"Available columns in the CSV: \", df.columns)\n",
    "\n",
    "            if 'Transkript' in df.columns:\n",
    "                df['Transkript'] = df['Transkript'].fillna('').astype(str)\n",
    "                transcript_data = \"\\n\".join(df['Transkript'].tolist())\n",
    "            else:\n",
    "                print(f\"Error: 'Transkript' column not found in the CSV.\")\n",
    "                return\n",
    "        elif file_name.endswith('.docx'):\n",
    "            transcript_data = read_docx(file_path)\n",
    "        elif file_name.endswith('.pdf'):\n",
    "            transcript_data = read_pdf(file_path)\n",
    "        else:\n",
    "            print(\"Unsupported file format.\")\n",
    "            return\n",
    "\n",
    "        # Debug Step 1: Print transcript_data\n",
    "        if transcript_data:\n",
    "            print(f\"Transcript Data: {transcript_data[:500]}\")  # Print first 500 characters to confirm\n",
    "        else:\n",
    "            print(\"No transcript data found.\")\n",
    "            return\n",
    "\n",
    "        # Debug Step 2: Track biography generation\n",
    "        summarizer = Summarizer()\n",
    "        biography = summarizer.generate_biography(transcript_data)\n",
    "        print(f\"Generated Biography: {biography}\")  # Print first 500 characters of biography\n",
    "\n",
    "        # Extending biography\n",
    "        extend_biography = summarizer.extend_biography(biography, transcript_data)\n",
    "        \n",
    "        # Removing incomplete sentences\n",
    "        final_biography = summarizer.remove_incomplete_sentence(extend_biography, transcript_data)\n",
    "\n",
    "        # Debug Step 3: Check final biography before saving to PDF\n",
    "        print(f\"Final Biography: {final_biography[:500]}\")  # Print first 500 characters of final biography\n",
    "\n",
    "        # Saving final biography to PDF\n",
    "        output_pdf_path = os.path.join('processed', file_name.rsplit('.', 1)[0] + '.pdf')\n",
    "        print(f\"PDF will be saved at: {os.path.abspath(output_pdf_path)}\")\n",
    "        save_text_to_pdf(final_biography, output_pdf_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "file_path = 'C:/Users/EDWIN SAMUEL/OneDrive/Desktop/Pipeline14/WG_ [EXTERN]  Transcripts and Biographies/adg0001_er_2024_04_23.csv'\n",
    "# file_path = '/home/jovyan/adg0001_er_2024_04_23.csv'\n",
    "process_file_and_update_status(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
