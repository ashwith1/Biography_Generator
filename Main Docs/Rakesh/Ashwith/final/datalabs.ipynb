{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install fpdf\n",
    "# %pip install transformers==4.43.1\n",
    "# %pip install vllm==0.5.3.post1\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# token = \"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\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from fpdf import FPDF\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_with_sprecher(df, sprecher_prefix):\n",
    "    df = df.dropna(subset=['Sprecher'])\n",
    "    filtered_rows = df[df['Sprecher'].str.startswith(sprecher_prefix)]\n",
    "    transkript_list = filtered_rows['Transkript'].tolist()\n",
    "    return transkript_list\n",
    "\n",
    "def transkript_to_string(transkript_list):\n",
    "    return \"\\n\".join(transkript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_chunks(text, max_words_per_chunk):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(word_tokenize(sentence))\n",
    "        if current_word_count + words_in_sentence > max_words_per_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_word_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += words_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "#Use this if the above chunking generates a lot of load on memory\n",
    "\n",
    "# def divide_into_chunks(text, max_words_per_chunk):\n",
    "#     words = word_tokenize(text)\n",
    "#     return [' '.join(words[i:i + max_words_per_chunk]) for i in range(0, len(words), max_words_per_chunk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Summarizer:\n",
    "#     def __init__(self):\n",
    "#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         self.tokenizer = None\n",
    "#         self.model = None\n",
    "#         self.temp_dir = tempfile.mkdtemp()  # Initialize temporary directory\n",
    "#         self.load_model()  # Ensure the model and tokenizer are loaded upon initialization\n",
    "        \n",
    "\n",
    "#     def load_model(self):\n",
    "#         gc.collect()  # Clear CPU memory\n",
    "\n",
    "#         # Ensure tokenizer and model are loaded only if not already set\n",
    "#         if self.tokenizer is None or self.model is None:\n",
    "#             print(\"Loading model...\")\n",
    "#             token = \"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\"\n",
    "#             try:\n",
    "#                 self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=token)\n",
    "#                 self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=token)\n",
    "#                 self.model = self.model.to(self.device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "#                 print(\"Model and tokenizer successfully loaded.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error loading model: {e}\")\n",
    "#                 raise\n",
    "#         else:\n",
    "#             print(\"Model already loaded and in use.\")\n",
    "\n",
    "\n",
    "#     def forward_pass(self, input_ids):\n",
    "#         \"\"\"Performs a forward pass using the model with the given input IDs.\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model.generate(\n",
    "#                 input_ids=input_ids,\n",
    "#                 max_length=self.max_tokens,\n",
    "#                 temperature=self.temperature,\n",
    "#                 top_k=self.top_k,\n",
    "#                 num_return_sequences=1\n",
    "#             )\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "#     def process_chunk(self, chunk, chunk_id):\n",
    "#         # Ensure the model is loaded\n",
    "#         self.load_model()\n",
    "\n",
    "#         input_ids = self.tokenizer(chunk, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "#         # Use `max_new_tokens` instead of `max_length` to limit the generation\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model.generate(\n",
    "#                 input_ids=input_ids,\n",
    "#                 max_new_tokens=self.max_tokens,  # Control the length of the generated output\n",
    "#                 temperature=self.temperature,\n",
    "#                 top_k=self.top_k,\n",
    "#                 num_return_sequences=1\n",
    "#             )\n",
    "\n",
    "#         generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         output_file = os.path.join(self.temp_dir, f\"chunk_{chunk_id}.txt\")\n",
    "#         with open(output_file, 'w') as f:\n",
    "#             f.write(generated_text)\n",
    "#         return output_file\n",
    "\n",
    "\n",
    "#     def process_all_chunks(self, chunk_prompts):\n",
    "#         \"\"\"Processes all chunks by invoking process_chunk for each one with retry logic.\"\"\"\n",
    "#         outputs = []\n",
    "#         for idx, prompt in enumerate(chunk_prompts):\n",
    "#             output = self.invoke_with_retry(self.process_chunk, prompt, chunk_id=idx)\n",
    "#             outputs.append(output)\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "\n",
    "#     def clear_model(self):\n",
    "#         print(\"Clearing model from memory...\")\n",
    "#         del self.model\n",
    "#         del self.tokenizer\n",
    "#         torch.cuda.empty_cache()  # Clear CUDA memory if used\n",
    "#         gc.collect()  # Force garbage collection to free memory\n",
    "\n",
    "#     def invoke_with_retry(self, func, *args, retries=3, **kwargs):\n",
    "#         for attempt in range(retries):\n",
    "#             try:\n",
    "#                 return func(*args, **kwargs)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "#                 if attempt == retries - 1:\n",
    "#                     raise  # Re-raise the exception if the last attempt fails\n",
    "\n",
    "\n",
    "\n",
    "#     def generate_biography(self, input_text):\n",
    "#         # Ensure the model is loaded\n",
    "#         self.load_model()\n",
    "        \n",
    "#         # Set specific parameters for this method\n",
    "#         self.temperature = 0.1\n",
    "#         self.max_tokens = 1024\n",
    "#         self.top_k = 1\n",
    "        \n",
    "#         prompt = \"\"\"\n",
    "#         Du bist ein deutsches Textzusammenfassungsmodell. Erstellen Sie eine prägnante Zusammenfassung des obigen Textes in deutscher Sprache innerhalb von 500 Wörtern. Konzentrieren Sie sich auf die wichtigsten Punkte und bewahren Sie Klarheit. Work only with the data given and do not provide your conclusions or interpretations of the biography or the data provided. Work only with the data.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         chunks = divide_into_chunks(input_text, max_words_per_chunk=70000)\n",
    "#         output_files = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "        \n",
    "#         full_biography = assemble_biography_from_disk(output_files)\n",
    "#         # self.clear_model()\n",
    "#         return full_biography\n",
    "\n",
    "\n",
    "\n",
    "#     def extend_biography(self, partial_biography, input_text):\n",
    "#         try:\n",
    "#             # Ensure the model is loaded\n",
    "#             self.load_model()\n",
    "\n",
    "#             # Set specific parameters for this method\n",
    "#             self.temperature = 0.1\n",
    "#             self.max_tokens = 800\n",
    "#             self.top_k = 1\n",
    "\n",
    "#             # Construct the prompt\n",
    "#             prompt = f\"\"\"\n",
    "#             Hier ist der erste Teil der Biografie: {partial_biography}\n",
    "#             Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "#             \"\"\"\n",
    "\n",
    "#             # Divide the input text into manageable chunks\n",
    "#             chunks = divide_into_chunks(input_text, max_words_per_chunk=25000)\n",
    "\n",
    "#             # Process all chunks and generate the extended biography\n",
    "#             output_files = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "\n",
    "#             # Assemble the full extended biography from the output files\n",
    "#             full_extended_biography = assemble_biography_from_disk(output_files)\n",
    "\n",
    "#             # Remove any incomplete sentences from the final output\n",
    "#             return self.remove_incomplete_sentence(full_extended_biography)\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred while extending the biography: {str(e)}\")\n",
    "#             return None\n",
    "        \n",
    "#         # finally:\n",
    "#         #     # Clear the model from memory to free resources\n",
    "#         #     self.clear_model()\n",
    "\n",
    "\n",
    "\n",
    "#     def refine_biography_to_500_words(self, extended_biography, input_text):\n",
    "#         try:\n",
    "#             # Ensure the model is loaded\n",
    "#             self.load_model()\n",
    "\n",
    "#             # Set specific parameters for this method\n",
    "#             self.temperature = 0.1\n",
    "#             self.max_tokens = 512\n",
    "#             self.top_k = 1\n",
    "\n",
    "#             # Construct the prompt\n",
    "#             prompt = f\"\"\"\n",
    "#             Hier ist der erste Teil der Biografie: {extended_biography}\n",
    "#             Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "#             \"\"\"\n",
    "\n",
    "#             # Divide the input text into manageable chunks\n",
    "#             chunks = divide_into_chunks(input_text, max_words_per_chunk=25000)\n",
    "\n",
    "#             # Process all chunks and generate the refined biography\n",
    "#             output_files = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "\n",
    "#             # Assemble the full refined biography from the output files\n",
    "#             full_refined_biography = assemble_biography_from_disk(output_files)\n",
    "\n",
    "#             # Remove any incomplete sentences from the final output\n",
    "#             return self.remove_incomplete_sentence(full_refined_biography)\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred while refining the biography: {str(e)}\")\n",
    "#             return None\n",
    "        \n",
    "#         finally:\n",
    "#             # Clear the model from memory to free resources\n",
    "#             self.clear_model()\n",
    "\n",
    "\n",
    "\n",
    "#     def remove_incomplete_sentence(self, biography):\n",
    "#         words = nltk.word_tokenize(biography)\n",
    "#         if len(words) <= 800:\n",
    "#             return biography\n",
    "        \n",
    "#         truncated_words = words[:800]\n",
    "#         truncated_text = \" \".join(truncated_words)\n",
    "#         last_full_stop_index = truncated_text.rfind('.')\n",
    "        \n",
    "#         if last_full_stop_index != -1:\n",
    "#             return truncated_text[:last_full_stop_index + 1]\n",
    "#         else:\n",
    "#             return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.temp_dir = tempfile.mkdtemp()  # Initialize temporary directory\n",
    "        self.load_model_once = False  # Flag to check if the model has been loaded\n",
    "\n",
    "    def load_model(self):\n",
    "        gc.collect()  # Clear CPU memory\n",
    "\n",
    "        if not self.load_model_once:\n",
    "            print(\"Loading model...\")\n",
    "            token = \"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\"\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=token)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=token)\n",
    "                self.model = self.model.to(self.device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "                print(\"Model and tokenizer successfully loaded.\")\n",
    "                self.load_model_once = True  # Set the flag to true after loading the model\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"Model already loaded and in use.\")\n",
    "\n",
    "    def forward_pass(self, input_ids):\n",
    "        \"\"\"Performs a forward pass using the model with the given input IDs.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        return outputs\n",
    "\n",
    "    def process_chunk(self, chunk, chunk_id):\n",
    "        self.load_model()\n",
    "        input_ids = self.tokenizer(chunk, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clear memory after processing each chunk\n",
    "        del input_ids, outputs\n",
    "        gc.collect()\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    def process_all_chunks(self, chunk_prompts):\n",
    "        \"\"\"Processes all chunks by invoking process_chunk for each one with retry logic.\"\"\"\n",
    "        outputs = []\n",
    "        for idx, prompt in enumerate(chunk_prompts):\n",
    "            output = self.invoke_with_retry(self.process_chunk, prompt, chunk_id=idx)\n",
    "            outputs.append(output)\n",
    "        return \" \".join(outputs)\n",
    "\n",
    "    def clear_model(self):\n",
    "        print(\"Clearing model from memory...\")\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()  # Clear CUDA memory if used\n",
    "        gc.collect()  # Force garbage collection to free memory\n",
    "\n",
    "    def invoke_with_retry(self, func, *args, retries=3, **kwargs):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise  # Re-raise the exception if the last attempt fails\n",
    "\n",
    "    def generate_biography(self, input_text):\n",
    "        self.load_model()\n",
    "        \n",
    "        self.temperature = 0.1\n",
    "        self.max_tokens = 512  # Lowered from 1024\n",
    "        self.top_k = 1\n",
    "        \n",
    "        prompt = \"\"\"\n",
    "        Du bist ein deutsches Textzusammenfassungsmodell. Erstellen Sie eine prägnante Zusammenfassung des obigen Textes in deutscher Sprache innerhalb von 500 Wörtern. Konzentrieren Sie sich auf die wichtigsten Punkte und bewahren Sie Klarheit. Work only with the data given and do not provide your conclusions or interpretations of the biography or the data provided. Work only with the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=10000)\n",
    "        output_biography = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "        \n",
    "        return output_biography\n",
    "\n",
    "    def extend_biography(self, partial_biography, input_text):\n",
    "        try:\n",
    "            self.load_model()\n",
    "\n",
    "            self.temperature = 0.1\n",
    "            self.max_tokens = 800\n",
    "            self.top_k = 1\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Hier ist der erste Teil der Biografie: {partial_biography}\n",
    "            Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "            \"\"\"\n",
    "\n",
    "            chunks = divide_into_chunks(input_text, max_words_per_chunk=10000)\n",
    "\n",
    "            extended_biography = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "\n",
    "            return self.remove_incomplete_sentence(extended_biography)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extending the biography: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def refine_biography_to_500_words(self, extended_biography, input_text):\n",
    "        try:\n",
    "            self.load_model()\n",
    "\n",
    "            self.temperature = 0.1\n",
    "            self.max_tokens = 512\n",
    "            self.top_k = 1\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Hier ist der erste Teil der Biografie: {extended_biography}\n",
    "            Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "            \"\"\"\n",
    "\n",
    "            chunks = divide_into_chunks(input_text, max_words_per_chunk=10000)\n",
    "\n",
    "            refined_biography = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "\n",
    "            return self.remove_incomplete_sentence(refined_biography)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while refining the biography: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        finally:\n",
    "            self.clear_model()\n",
    "\n",
    "    def remove_incomplete_sentence(self, biography):\n",
    "        words = nltk.word_tokenize(biography)\n",
    "        if len(words) <= 800:\n",
    "            return biography\n",
    "        \n",
    "        truncated_words = words[:800]\n",
    "        truncated_text = \" \".join(truncated_words)\n",
    "        last_full_stop_index = truncated_text.rfind('.')\n",
    "        \n",
    "        if last_full_stop_index != -1:\n",
    "            return truncated_text[:last_full_stop_index + 1]\n",
    "        else:\n",
    "            return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_biography_from_disk(output_files):\n",
    "    full_biography = []\n",
    "    for output_file in output_files:\n",
    "        with open(output_file, 'r') as f:\n",
    "            full_biography.append(f.read().strip())\n",
    "\n",
    "    return \" \".join(full_biography)\n",
    "\n",
    "\n",
    "def cleanup_temp_files(temp_dir):\n",
    "    shutil.rmtree(temp_dir)  # Remove the temporary directory and its contents\n",
    "\n",
    "\n",
    "def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def save_text_to_pdf(text, pdf_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    \n",
    "    pdf.output(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_file(file_path):\n",
    "#     summarizer = Summarizer()\n",
    "    \n",
    "#     if file_path.endswith('.csv'):\n",
    "#         df = read_csv(file_path)\n",
    "#         if df is not None:\n",
    "#             sprecher_prefix = 'IP_'\n",
    "#             transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "#             transcript_data = transkript_to_string(transkript_list)\n",
    "#         else:\n",
    "#             print(\"Failed to read CSV file.\")\n",
    "#             return\n",
    "#     else:\n",
    "#         print(\"Unsupported file format.\")\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files_in_directory(directory_path):\n",
    "    summarizer = Summarizer()\n",
    "    output_directory = os.path.join(directory_path, \"output_pdfs\")\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if not file_name.endswith('.csv'):\n",
    "            print(f\"Unsupported file format: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        df = read_csv(file_path)\n",
    "        \n",
    "        if df is not None:\n",
    "            sprecher_prefix = 'IP_'\n",
    "            transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "            transcript_data = transkript_to_string(transkript_list)\n",
    "            \n",
    "            print(\"Generating initial biography...\")\n",
    "            initial_biography = summarizer.generate_biography(transcript_data)\n",
    "            print(\"Initial biography generated:\")\n",
    "            print(initial_biography)\n",
    "            \n",
    "            print(\"Extending biography...\")\n",
    "            extended_biography = summarizer.extend_biography(initial_biography, transcript_data)\n",
    "            print(\"Biography extended:\")\n",
    "            print(extended_biography)\n",
    "\n",
    "            print(\"Refining biography...\")\n",
    "            refined_biography = summarizer.refine_biography_to_500_words(extended_biography, transcript_data)\n",
    "            print(\"Biography refined.\")\n",
    "            print(refined_biography)\n",
    "            \n",
    "            output_pdf_path = os.path.join(output_directory, file_name.replace('.csv', '.pdf'))\n",
    "            save_text_to_pdf(refined_biography.strip(), output_pdf_path)\n",
    "            print(f\"Processed and saved {file_name} as PDF.\")\n",
    "            \n",
    "            summarizer.clear_model()\n",
    "        else:\n",
    "            print(f\"Failed to read CSV file: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling block to analyze performance\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/\"\n",
    "\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    process_all_files_in_directory(directory_path)\n",
    "\n",
    "    profiler.disable()\n",
    "    stream = io.StringIO()\n",
    "    stats = pstats.Stats(profiler, stream=stream).sort_stats('cumulative')\n",
    "    stats.print_stats()\n",
    "    print(stream.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
