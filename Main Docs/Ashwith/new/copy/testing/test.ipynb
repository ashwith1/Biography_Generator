{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install reportlab\n",
    "# %pip install nltk\n",
    "# %pip install langchain-together\n",
    "# %pip install loguru\n",
    "# %pip install sentence-transformers scikit-learn\n",
    "#%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asha4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\asha4\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from reportlab.lib.pagesizes import letter, A4\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import simpleSplit\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from loguru import logger\n",
    "from langchain_together import Together\n",
    "from tqdm.notebook import tqdm  # Progress bar\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asha4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "logger.add(\"biography_generator.log\", rotation=\"1 MB\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOGETHER_API_KEYS = [\n",
    "    \"cc242f9a074c36a00aeded59331d7c67780ed6078a9481c0dc35b2d95831f2c7\",\n",
    "    \"89f6359c5009e4d13ca731bb9085bf686123aa816e64ede39a6b295143c086c3\",\n",
    "    \"104dd97ba7c157e74fd5bda4afcad7774ff340adeb30773c3c0a7639e4fae45e\",\n",
    "    \"126022cbbf2d4e73287470a6cafe29a87a3423b0c0511c2a68c9da83f16f2665\"\n",
    "    # Add more API keys as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read CSV and extract data\n",
    "def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        logger.info(f\"Successfully read CSV file: {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV file: {file_path} - {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract rows with specific prefix in 'Sprecher' column\n",
    "def extract_rows_with_sprecher(df, sprecher_prefix):\n",
    "    try:\n",
    "        df = df.dropna(subset=['Sprecher'])\n",
    "        filtered_rows = df[df['Sprecher'].str.startswith(sprecher_prefix)]\n",
    "        transkript_list = filtered_rows['Transkript'].tolist()\n",
    "        logger.info(f\"Extracted rows with Sprecher starting with '{sprecher_prefix}'\")\n",
    "        return transkript_list\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting rows: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to convert list to string\n",
    "def transkript_to_string(transkript_list):\n",
    "    return \"\\n\".join(transkript_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the number of tokens in a text\n",
    "def count_tokens(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Function to divide text into chunks within the token limit\n",
    "def divide_into_chunks(text, max_words_per_chunk, max_tokens):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "    current_token_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(sentence.split())\n",
    "        tokens_in_sentence = count_tokens(sentence)\n",
    "        if current_word_count + words_in_sentence > max_words_per_chunk or current_token_count + tokens_in_sentence > max_tokens:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_word_count = words_in_sentence\n",
    "            current_token_count = tokens_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += words_in_sentence\n",
    "            current_token_count += tokens_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Adjusted chunk size to provide more context for detailed summaries\n",
    "MAX_WORDS_PER_CHUNK = 1500  # Reduced chunk size to stay within token limits\n",
    "MAX_TOKENS = 3073  # 4097 (max tokens) - 1024 (max new tokens) = 3073\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Summarizer class with multiple API keys support\n",
    "class Summarizer:\n",
    "    def __init__(self, api_keys):\n",
    "        self.api_keys = api_keys\n",
    "        self.current_key_index = 0\n",
    "        self.llm = self.create_llm(api_keys[self.current_key_index])\n",
    "        self.llm1 = self.create_llm(api_keys[self.current_key_index])\n",
    "\n",
    "    def create_llm(self, api_key):\n",
    "        return Together(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "            temperature=0.7,\n",
    "            max_tokens=1024,\n",
    "            top_k=1,\n",
    "            together_api_key=api_key\n",
    "        )\n",
    "\n",
    "    def switch_api_key(self):\n",
    "        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)\n",
    "        new_key = self.api_keys[self.current_key_index]\n",
    "        self.llm = self.create_llm(new_key)\n",
    "        self.llm1 = self.create_llm(new_key)\n",
    "        logger.info(f\"Switched to new API key: {new_key}\")\n",
    "\n",
    "    def generate_biography(self, input_text):\n",
    "        prompt_template = \"\"\"\n",
    "Sie sind ein professioneller deutscher Biografieschreiber. Fassen Sie den folgenden Text prägnant zusammen. Die Zusammenfassung sollte mindestens 400 Wörter umfassen und alle wichtigen Details enthalten.\n",
    "\n",
    "Beispielzusammenfassung:\n",
    "\"Marie Curie wurde 1867 in Warschau geboren. Sie war eine polnische und französische Physikerin und Chemikerin, die für ihre bahnbrechende Forschung im Bereich der Radioaktivität bekannt ist. Sie erhielt zwei Nobelpreise, einen in Physik und einen in Chemie. Curie entdeckte die Elemente Radium und Polonium und entwickelte die Theorie der Radioaktivität. Sie gründete das Curie-Institut in Paris und in Warschau, das bis heute ein bedeutendes Zentrum für medizinische Forschung ist. Curie starb 1934 an aplastischer Anämie, die durch ihre langjährige Exposition gegenüber radioaktiven Materialien verursacht wurde.\"\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Zusammenfassung:\n",
    "\"\"\"\n",
    "        prompt = prompt_template.format(text=input_text)\n",
    "        full_input = prompt\n",
    "        max_retries = 3  # Maximum number of retries\n",
    "        retry_delay = 30  # Initial delay in seconds\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                output_summary = self.llm.invoke(full_input)\n",
    "                if len(output_summary.split()) < 400:  # Check if the output is shorter than expected\n",
    "                    logger.info(\"Output too short, retrying with a modified prompt...\")\n",
    "                    full_input = prompt + \"\\nBitte geben Sie mehr Details.\"\n",
    "                    time.sleep(retry_delay)  # Wait before retrying\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                    continue  # Retry with a modified prompt if the output is too short\n",
    "                return output_summary\n",
    "            except ValueError as e:\n",
    "                if \"credit limit exceeded\" in str(e).lower():\n",
    "                    logger.warning(f\"Credit limit exceeded. Switching API key and retrying...\")\n",
    "                    self.switch_api_key()\n",
    "                elif \"rate limited\" in str(e).lower():\n",
    "                    logger.warning(f\"Rate limit exceeded. Retrying in {retry_delay} seconds (attempt {attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                elif \"input validation error\" in str(e).lower():\n",
    "                    logger.error(f\"Input validation error: {e}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logger.error(f\"An error occurred: {e}\")\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logger.error(f\"An error occurred: {e}\")\n",
    "                raise\n",
    "\n",
    "        logger.error(\"Max retries reached. Failed to generate a detailed biography.\")\n",
    "        return \"Die Zusammenfassung ist zu kurz. Weitere Details konnten nicht abgerufen werden.\"\n",
    "\n",
    "    def final_biography(self, input_text):\n",
    "        prompt1 = \"\"\"\n",
    "Sie sind ein professioneller deutscher Biografieschreiber. Verfassen Sie eine detaillierte Biografie des obigen Textes. Konzentrieren Sie sich auf alle wichtigen Ereignisse und vermeiden Sie Wiederholungen.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\n",
    "Biografie:\n",
    "\"\"\"\n",
    "        full_input = prompt1.format(text=input_text)\n",
    "        max_retries = 5  # Maximum number of retries\n",
    "        retry_delay = 30  # Initial delay in seconds\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                output_summary = self.llm1.invoke(full_input)\n",
    "                return output_summary\n",
    "            except ValueError as e:\n",
    "                if \"credit limit exceeded\" in str(e).lower():\n",
    "                    logger.warning(f\"Credit limit exceeded. Switching API key and retrying...\")\n",
    "                    self.switch_api_key()\n",
    "                elif \"rate limited\" in str(e).lower():\n",
    "                    logger.warning(f\"Rate limit exceeded. Retrying in {retry_delay} seconds (attempt {attempt + 1}/{max_retries})...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay = min(retry_delay * 2, 300)  # Exponential backoff, cap at 5 minutes\n",
    "                elif \"input validation error\" in str(e).lower():\n",
    "                    logger.error(f\"Input validation error: {e}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logger.error(f\"An error occurred: {e}\")\n",
    "                    raise\n",
    "            except Exception as e:\n",
    "                logger.error(f\"An error occurred: {e}\")\n",
    "                raise\n",
    "\n",
    "        logger.error(\"Max retries reached. Failed to generate the final biography.\")\n",
    "        return \"Die Biografie konnte nicht vollständig erstellt werden.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicate sentences using clustering\n",
    "def remove_duplicates(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return text\n",
    "    embeddings = model.encode(sentences)\n",
    "    clustering = DBSCAN(eps=0.3, min_samples=1, metric='cosine').fit(embeddings)  # Adjusted eps for less aggressive clustering\n",
    "    unique_sentences = []\n",
    "    seen_clusters = set()\n",
    "    for cluster_id in clustering.labels_:\n",
    "        if cluster_id not in seen_clusters:\n",
    "            cluster_indices = [i for i, label in enumerate(clustering.labels_) if label == cluster_id]\n",
    "            if cluster_indices:\n",
    "                representative_sentence = sentences[cluster_indices[0]]\n",
    "                unique_sentences.append(representative_sentence)\n",
    "                seen_clusters.add(cluster_id)\n",
    "    return ' '.join(unique_sentences)\n",
    "\n",
    "# Stateless function to generate biography\n",
    "def generate_biography(input_text, summarizer):\n",
    "    chunks = divide_into_chunks(input_text, MAX_WORDS_PER_CHUNK, MAX_TOKENS)\n",
    "    summaries = []\n",
    "    chunk_number = 1\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=\"Generating biography\"):\n",
    "        logger.info(f\"Processing chunk {chunk_number}\")\n",
    "        chunk_summary = summarizer.generate_biography(chunk)\n",
    "        logger.debug(f\"Generated chunk summary: {chunk_summary}\")\n",
    "        chunk_summary = remove_duplicates(chunk_summary)\n",
    "        logger.debug(f\"Chunk summary after removing duplicates: {chunk_summary}\")\n",
    "        print(f\"Chunk {chunk_number} output after removing duplicates:\\n{chunk_summary}\\n\")\n",
    "        summaries.append(chunk_summary)\n",
    "        chunk_number += 1\n",
    "\n",
    "    combined_summary = \"\\n\".join(summaries)\n",
    "    return combined_summary\n",
    "\n",
    "# Function to generate final biography (remains the same)\n",
    "def final_biography(summarizer, input_text):\n",
    "    chunks = divide_into_chunks(input_text, MAX_WORDS_PER_CHUNK, MAX_TOKENS)\n",
    "    summary = \"\"\n",
    "    chunk_number = 1\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=\"Generating final biography\"):\n",
    "        logger.info(f\"Processing final chunk {chunk_number}\")\n",
    "        chunk_summary = summarizer.final_biography(chunk)\n",
    "        logger.debug(f\"Generated final chunk summary: {chunk_summary}\")\n",
    "        chunk_summary = remove_duplicates(chunk_summary)\n",
    "        logger.debug(f\"Final chunk summary after removing duplicates: {chunk_summary}\")\n",
    "        print(f\"Final chunk {chunk_number} output after removing duplicates:\\n{chunk_summary}\\n\")\n",
    "        summary += chunk_summary + \"\\n\"\n",
    "        chunk_number += 1\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save biography to PDF\n",
    "def save_biography_to_pdf(text, original_csv_filename):\n",
    "    try:\n",
    "        timestamp = int(time.time())\n",
    "        base_name = os.path.splitext(os.path.basename(original_csv_filename))[0]\n",
    "        filename = f\"{base_name}_{timestamp}.pdf\"\n",
    "        \n",
    "        c = canvas.Canvas(filename, pagesize=A4)\n",
    "        text = text.split('\\n')\n",
    "        text = [line.strip() for line in text if line.strip() != '']\n",
    "        width, height = A4\n",
    "        left_margin = 72\n",
    "        right_margin = width - 72\n",
    "        y = height - 72  # Start writing from the top of the page\n",
    "        max_width = right_margin - left_margin\n",
    "        \n",
    "        for line in text:\n",
    "            wrapped_lines = simpleSplit(line, \"Helvetica\", 12, max_width)\n",
    "            for wrapped_line in wrapped_lines:\n",
    "                c.drawString(left_margin, y, wrapped_line)\n",
    "                y -= 15  # Move to the next line\n",
    "                if y < 72:\n",
    "                    c.showPage()  # Add a new page if the current is full\n",
    "                    y = height - 72  # Reset y coordinate\n",
    "        \n",
    "        c.save()\n",
    "        logger.info(f\"Biography saved to {filename}\")\n",
    "        return filename\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while saving the PDF: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main workflow\n",
    "\n",
    "# Use widgets for interactive parameter input\n",
    "csv_file_path_widget = widgets.Text(\n",
    "    value=\"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/adg0001_er_2024_04_23.csv\",\n",
    "    description=\"CSV File Path:\",\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "sprecher_prefix_widget = widgets.Text(\n",
    "    value='IP_',\n",
    "    description=\"Sprecher Prefix:\"\n",
    ")\n",
    "\n",
    "display(csv_file_path_widget, sprecher_prefix_widget)\n",
    "\n",
    "def run_workflow(csv_file_path, sprecher_prefix):\n",
    "    df = read_csv(csv_file_path)\n",
    "\n",
    "    if df is not None:\n",
    "        transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "        transcript_data = transkript_to_string(transkript_list)\n",
    "\n",
    "        summarizer = Summarizer(api_keys=TOGETHER_API_KEYS)\n",
    "        \n",
    "        # Generate summaries for each chunk\n",
    "        chunk_summaries = generate_biography(transcript_data, summarizer)\n",
    "        \n",
    "        # Combine summaries into one text\n",
    "        combined_summary = \"\\n\".join(chunk_summaries)\n",
    "        \n",
    "        # Generate final biography from combined summary\n",
    "        final_bio = final_biography(summarizer, combined_summary)\n",
    "\n",
    "        # Save the final biography to a PDF file\n",
    "        filename = save_biography_to_pdf(final_bio, csv_file_path)\n",
    "        if filename:\n",
    "            logger.info(f\"Biography saved to {filename}\")\n",
    "            print(f\"Biography saved to {filename}\")\n",
    "        else:\n",
    "            logger.error(\"Failed to save biography to PDF.\")\n",
    "    else:\n",
    "        logger.error(\"Failed to read the CSV file.\")\n",
    "\n",
    "# Button to execute the workflow\n",
    "run_button = widgets.Button(description=\"Run Workflow\")\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    run_workflow(csv_file_path_widget.value, sprecher_prefix_widget.value)\n",
    "\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "display(run_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/adg0001_er_2024_04_23.csv\",\n",
    "# value = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/adg0002_er_2024_04_23.csv\","
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
