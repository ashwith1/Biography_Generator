{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the terminal after running the two blocks below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install fpdf\n",
    "# %pip install transformers==4.43.1\n",
    "# %pip install vllm==0.5.3.post1\n",
    "# %pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# token = \"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\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", use_auth_token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import gc\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from fpdf import FPDF\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_with_sprecher(df, sprecher_prefix):\n",
    "    df = df.dropna(subset=['Sprecher'])\n",
    "    filtered_rows = df[df['Sprecher'].str.startswith(sprecher_prefix)]\n",
    "    transkript_list = filtered_rows['Transkript'].tolist()\n",
    "    return transkript_list\n",
    "\n",
    "def transkript_to_string(transkript_list):\n",
    "    return \"\\n\".join(transkript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_chunks(text, max_words_per_chunk):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(word_tokenize(sentence))\n",
    "        if current_word_count + words_in_sentence > max_words_per_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_word_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += words_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "#Use this if the above chunking generates a lot of load on memory\n",
    "\n",
    "# def divide_into_chunks(text, max_words_per_chunk):\n",
    "#     words = word_tokenize(text)\n",
    "#     return [' '.join(words[i:i + max_words_per_chunk]) for i in range(0, len(words), max_words_per_chunk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.temp_dir = tempfile.mkdtemp()  # Initialize temporary directory\n",
    "        self.load_model_once = False  # Flag to check if the model has been loaded\n",
    "\n",
    "    def load_model(self):\n",
    "        gc.collect()  # Clear CPU memory\n",
    "\n",
    "        if not self.load_model_once:\n",
    "            print(\"Loading model...\")\n",
    "            token = \"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\"\n",
    "            try:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=token)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", token=token)\n",
    "                self.model = self.model.to(self.device)  # Move the model to the appropriate device (CPU or GPU)\n",
    "                print(\"Model and tokenizer successfully loaded.\")\n",
    "                self.load_model_once = True  # Set the flag to true after loading the model\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            print(\"Model already loaded and in use.\")\n",
    "\n",
    "    def forward_pass(self, input_ids):\n",
    "        \"\"\"Performs a forward pass using the model with the given input IDs.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        return outputs\n",
    "\n",
    "    def process_chunk(self, chunk, chunk_id):\n",
    "        self.load_model()\n",
    "        input_ids = self.tokenizer(chunk, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_new_tokens=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                top_k=self.top_k,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clear memory after processing each chunk\n",
    "        del input_ids, outputs\n",
    "        gc.collect()\n",
    "\n",
    "        return generated_text\n",
    "\n",
    "    def process_all_chunks(self, chunk_prompts):\n",
    "        \"\"\"Processes all chunks by invoking process_chunk for each one with retry logic.\"\"\"\n",
    "        outputs = []\n",
    "        for idx, prompt in enumerate(chunk_prompts):\n",
    "            output = self.invoke_with_retry(self.process_chunk, prompt, chunk_id=idx)\n",
    "            outputs.append(output)\n",
    "        return \" \".join(outputs)\n",
    "\n",
    "    def clear_model(self):\n",
    "        print(\"Clearing model from memory...\")\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        torch.cuda.empty_cache()  # Clear CUDA memory if used\n",
    "        gc.collect()  # Force garbage collection to free memory\n",
    "\n",
    "    def invoke_with_retry(self, func, *args, retries=3, **kwargs):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise  # Re-raise the exception if the last attempt fails\n",
    "\n",
    "    def generate_biography(self, input_text):\n",
    "        self.load_model()\n",
    "        \n",
    "        self.temperature = 0.1\n",
    "        self.max_tokens = 1024\n",
    "        self.top_k = 1\n",
    "        \n",
    "        prompt = \"\"\"\n",
    "\n",
    "I would like you to generate biography of an interviewee based on the following structured questions in German language and  written in third-person-view, make sure it is in german language only. Please address each question thoroughly, ensuring that the narrative flows smoothly from one life stage to the next. The biography should include the following information:\n",
    "\n",
    "Birth and Early Family Life:\n",
    "\n",
    "When and where was the interviewee born? Include the date and location of birth.\n",
    "Who are the interviewee's parents? Provide their names, backgrounds, and any relevant details about their lives.\n",
    "Does the interviewee have any siblings? If so, provide details about them, including names and relationships.\n",
    "Education:\n",
    "\n",
    "Which school or schools did the interviewee attend? Mention the date, names of the institutions, locations, and any significant experiences or achievements during their education.\n",
    "Career and Professional Life:\n",
    "\n",
    "What profession did the interviewee learn or train for? Mention date and describe the nature of their training or education in this field.\n",
    "Which jobs or professions has the interviewee practiced? Include details about the dates, roles, companies, or organizations they worked for, and any significant milestones or achievements in their career.\n",
    "Life Events and Personal Milestones:\n",
    "\n",
    "What were the formative or significant life events with years mentioned in the interviewee's childhood? Mention dates, Include any experiences that had a lasting impact.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adolescence?  Mention dates, Describe how these events influenced their path in life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's early adult years? Include details about any dates, transitions, challenges, or accomplishments during this period.\n",
    "What were the formative or significant life events with years mentioned during the interviewee's adult years?  Mention dates, Describe key experiences that shaped their personal or professional life.\n",
    "What were the formative or significant life events with years mentioned in the interviewee's late adult years? Highlight any dates, major changes, achievements, or reflections during this time.\n",
    "Personal Life:\n",
    "\n",
    "Did the interviewee marry  with years mentioned? If yes, provide details about their spouse,dates, including the name and any significant information about their relationship.\n",
    "Does the interviewee have children with years mentioned? If so, provide details about their children, dates, including names and any significant life events related to them.\n",
    "Significant Life Events:\n",
    "\n",
    "What are the most significant life events that have shaped the interviewee's life with years mentioned? Reflect on how these events with years mentioned impacted their personal growth, relationships, or career, dates.\n",
    "Please ensure the biography is coherent, chronological, detailed, and presents a well-rounded view of the interviewee's life journey with years mentioned. Include years, don't forget any years mentioned in the interview.\n",
    "   \n",
    "           \"\"\"\n",
    "        \n",
    "        chunks = divide_into_chunks(input_text, max_words_per_chunk=70000)\n",
    "        output_biography = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "        \n",
    "        return output_biography\n",
    "\n",
    "    def extend_biography(self, partial_biography, input_text):\n",
    "        try:\n",
    "            self.load_model()\n",
    "\n",
    "            self.temperature = 0.1\n",
    "            self.max_tokens = 800\n",
    "            self.top_k = 1\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Hier ist der erste Teil der Biografie: {partial_biography}\n",
    "            Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "            \"\"\"\n",
    "\n",
    "            chunks = divide_into_chunks(input_text, max_words_per_chunk=70000)\n",
    "\n",
    "            extended_biography = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "\n",
    "            return self.remove_incomplete_sentence(extended_biography)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while extending the biography: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def refine_biography_to_500_words(self, extended_biography, input_text):\n",
    "        try:\n",
    "            self.load_model()\n",
    "\n",
    "            self.temperature = 0.1\n",
    "            self.max_tokens = 512\n",
    "            self.top_k = 1\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Hier ist der erste Teil der Biografie: {extended_biography}\n",
    "            Nun sehen Sie sich die Daten erneut an und ergänzen Sie die Biografie um die fehlenden Informationen.\n",
    "            \"\"\"\n",
    "\n",
    "            chunks = divide_into_chunks(input_text, max_words_per_chunk=70000)\n",
    "\n",
    "            refined_biography = self.process_all_chunks([chunk + prompt for chunk in chunks])\n",
    "\n",
    "            return self.remove_incomplete_sentence(refined_biography)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while refining the biography: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        finally:\n",
    "            self.clear_model()\n",
    "\n",
    "    def remove_incomplete_sentence(self, biography):\n",
    "        words = nltk.word_tokenize(biography)\n",
    "        if len(words) <= 800:\n",
    "            return biography\n",
    "        \n",
    "        truncated_words = words[:800]\n",
    "        truncated_text = \" \".join(truncated_words)\n",
    "        last_full_stop_index = truncated_text.rfind('.')\n",
    "        \n",
    "        if last_full_stop_index != -1:\n",
    "            return truncated_text[:last_full_stop_index + 1]\n",
    "        else:\n",
    "            return truncated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_biography_from_disk(output_files):\n",
    "    full_biography = []\n",
    "    for output_file in output_files:\n",
    "        with open(output_file, 'r') as f:\n",
    "            full_biography.append(f.read().strip())\n",
    "\n",
    "    return \" \".join(full_biography)\n",
    "\n",
    "\n",
    "def cleanup_temp_files(temp_dir):\n",
    "    shutil.rmtree(temp_dir)  # Remove the temporary directory and its contents\n",
    "\n",
    "\n",
    "def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "def save_text_to_pdf(text, pdf_path):\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", size=12)\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        pdf.multi_cell(0, 10, line)\n",
    "    \n",
    "    pdf.output(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files_in_directory(directory_path):\n",
    "    summarizer = Summarizer()\n",
    "    output_directory = os.path.join(directory_path, \"output_pdfs\")\n",
    "    \n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if not file_name.endswith('.csv'):\n",
    "            print(f\"Unsupported file format: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        df = read_csv(file_path)\n",
    "        \n",
    "        if df is not None:\n",
    "            sprecher_prefix = 'IP_'\n",
    "            transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "            transcript_data = transkript_to_string(transkript_list)\n",
    "            \n",
    "            print(\"Generating initial biography...\")\n",
    "            initial_biography = summarizer.generate_biography(transcript_data)\n",
    "            print(\"Initial biography generated:\")\n",
    "            print(initial_biography)\n",
    "            \n",
    "            # Uncomment the below two if shorter output is necessary\n",
    "            # print(\"Extending biography...\")\n",
    "            # extended_biography = summarizer.extend_biography(initial_biography, transcript_data)\n",
    "            # print(\"Biography extended:\")\n",
    "            # print(extended_biography)\n",
    "\n",
    "            # print(\"Refining biography...\")\n",
    "            # refined_biography = summarizer.refine_biography_to_500_words(extended_biography, transcript_data)\n",
    "            # print(\"Biography refined.\")\n",
    "            # print(refined_biography)\n",
    "            \n",
    "            output_pdf_path = os.path.join(output_directory, file_name.replace('.csv', '.pdf'))\n",
    "            save_text_to_pdf(initial_biography.strip(), output_pdf_path)\n",
    "            print(f\"Processed and saved {file_name} as PDF.\")\n",
    "            \n",
    "            summarizer.clear_model()\n",
    "        else:\n",
    "            print(f\"Failed to read CSV file: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling block to analyze performance\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/\"  #provide the path to the directory containing the CSV files\n",
    "\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "\n",
    "    process_all_files_in_directory(directory_path)\n",
    "\n",
    "    profiler.disable()\n",
    "    stream = io.StringIO()\n",
    "    stats = pstats.Stats(profiler, stream=stream).sort_stats('cumulative')\n",
    "    stats.print_stats()\n",
    "    print(stream.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
