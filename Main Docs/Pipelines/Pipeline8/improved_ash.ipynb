{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asha4\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from langchain_together import Together\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import simpleSplit\n",
    "from docx import Document\n",
    "import os\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract rows from a DataFrame where 'Sprecher' column starts with the specified prefix.\n",
    "def extract_rows_with_sprecher(df, sprecher_prefix):\n",
    "    df = df.dropna(subset=['Sprecher'])\n",
    "    filtered_rows = df[df['Sprecher'].str.startswith(sprecher_prefix)]\n",
    "    transkript_list = filtered_rows['Transkript'].tolist()\n",
    "    return transkript_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the 'Transkript' column to a single string.\n",
    "def transkript_to_string(transkript_list):\n",
    "    return \"\\n\".join(transkript_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide text into chunks close to a specified maximum number of words without cutting through sentences.\n",
    "def divide_into_chunks(text, max_words_per_chunk):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words_in_sentence = len(word_tokenize(sentence))\n",
    "        if current_word_count + words_in_sentence > max_words_per_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_word_count = words_in_sentence\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += words_in_sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer:\n",
    "    def __init__(self):\n",
    "        self.llm = Together(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            temperature=0.4,\n",
    "            max_tokens= 2048,\n",
    "            top_k=1,\n",
    "            together_api_key=\"809750d4770635c394317b2afb1baefa7173070c968384d8ed26c9a0a1e72a10\"\n",
    "        )\n",
    "        self.llm2 = Together(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            temperature=0.4,\n",
    "            max_tokens= 512,\n",
    "            top_k=1,\n",
    "            together_api_key=\"809750d4770635c394317b2afb1baefa7173070c968384d8ed26c9a0a1e72a10\"\n",
    "        )\n",
    "        self.llm1 = Together(\n",
    "            model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            temperature=0.4,\n",
    "            max_tokens= 300,\n",
    "            top_k=1,\n",
    "            together_api_key=\"809750d4770635c394317b2afb1baefa7173070c968384d8ed26c9a0a1e72a10\"\n",
    "        )\n",
    "\n",
    "    def generate_biography(self, input_text):\n",
    "        prompt = \"\"\"\n",
    "    Du bist ein deutsches Textzusammenfassungsmodell. Erstellen Sie eine prägnante Zusammenfassung des obigen Textes in deutscher Sprache innerhalb von 500 Wörtern. Konzentrieren Sie sich auf die wichtigsten Punkte und bewahren Sie Klarheit. Mention years within the biography. Generate only on data given to the bioraphy and do not give your own opinions as an add on to the biography.\n",
    "        \"\"\"\n",
    "        full_input = input_text + prompt\n",
    "        retry_delay = 30\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                output_summary = self.llm.invoke(full_input)\n",
    "                return output_summary\n",
    "            except Exception as e:\n",
    "                if \"rate limited\" in str(e).lower():\n",
    "                    print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds...\")\n",
    "                    asyncio.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    raise\n",
    "\n",
    "    def final_biography(self, input_text):\n",
    "        prompt1 = \"\"\"\n",
    "Bitte erstellen Sie eine Biografie auf Basis des untenstehenden Textes in deutscher Sprache mit maximal 500 Wörtern. Generate only on data given and do not give your opinions.\n",
    "        \"\"\"\n",
    "        full_input = input_text + prompt1\n",
    "        retry_delay = 30\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                output_summary = self.llm1.invoke(full_input)\n",
    "                return output_summary\n",
    "            except Exception as e:\n",
    "                if \"rate limited\" in str(e).lower():\n",
    "                    print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds...\")\n",
    "                    asyncio.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    raise\n",
    "\n",
    "    def refine_biography(self, input_text):\n",
    "        def truncate_to_word_limit(text, limit):\n",
    "            words = word_tokenize(text)\n",
    "            if len(words) > limit:\n",
    "                words = words[:limit]\n",
    "            return ' '.join(words)\n",
    "\n",
    "        words = word_tokenize(input_text)\n",
    "\n",
    "        if len(words) <= 500:\n",
    "            return input_text\n",
    "\n",
    "        sentences = sent_tokenize(input_text)\n",
    "        refined_text = \"\"\n",
    "        current_word_count = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_word_count = len(word_tokenize(sentence))\n",
    "            if current_word_count + sentence_word_count <= 500:\n",
    "                refined_text += sentence + \" \"\n",
    "                current_word_count += sentence_word_count\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        refined_text = refined_text.strip()\n",
    "        refined_text = truncate_to_word_limit(refined_text, 500)\n",
    "\n",
    "        return refined_text\n",
    "\n",
    "    def final_2_biography(self, input_text):\n",
    "        prompt = \"\"\"\n",
    "        Du bist ein deutsches Textzusammenfassungsmodell. Erstellen Sie eine prägnante Zusammenfassung des oben genannten Textes auf Deutsch innerhalb von 500 Wörtern und behalten Sie alle wichtigen Daten bei. Konzentrieren Sie sich auf die wichtigsten Punkte und bewahren Sie Klarheit.\n",
    "        \"\"\"\n",
    "        full_input = input_text + prompt\n",
    "        retry_delay = 30\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                output_summary = self.llm2.invoke(full_input)\n",
    "                return output_summary\n",
    "            except Exception as e:\n",
    "                if \"rate limited\" in str(e).lower():\n",
    "                    print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds...\")\n",
    "                    asyncio.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a biography from text chunks.\n",
    "async def generate_biography(summarizer, input_text):\n",
    "    chunks = divide_into_chunks(input_text, 50000)\n",
    "    summary = \"\"\n",
    "    chunk_number = 1\n",
    "\n",
    "    for chunk in chunks:\n",
    "        print(f\"Chunk {chunk_number}:\")\n",
    "        print(chunk)\n",
    "        summary += summarizer.generate_biography(chunk) + \"\\n\"\n",
    "        chunk_number += 1\n",
    "    return summary\n",
    "\n",
    "# Function to read data from a CSV file.\n",
    "async def read_csv(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to read data from a DOCX file.\n",
    "async def read_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        data = [p.text for p in doc.paragraphs if p.text]\n",
    "        return \" \".join(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading DOCX file: {e}\")\n",
    "        return None\n",
    "\n",
    "async def final_biography(summarizer, input_text):\n",
    "    chunks = divide_into_chunks(input_text, 1000)\n",
    "    summary = \"\"\n",
    "    chunk_number = 1\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer.final_biography(chunk) + \"\\n\"\n",
    "        chunk_number += 1\n",
    "    return summary\n",
    "\n",
    "def count_words(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return len(words)\n",
    "\n",
    "async def final_2_biography(summarizer, input_text):\n",
    "    chunks = divide_into_chunks(input_text, 50000)\n",
    "    summary = \"\"\n",
    "    chunk_number = 1\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary += summarizer.final_2_biography(chunk) + \"\\n\"\n",
    "        chunk_number += 1\n",
    "    return summary\n",
    "\n",
    "def remove_incomplete_sentence(biography):\n",
    "    last_full_stop_index = biography.rfind('.')\n",
    "    if last_full_stop_index != -1:\n",
    "        return biography[:last_full_stop_index + 1]\n",
    "    else:\n",
    "        return biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asha4\\AppData\\Local\\Temp\\ipykernel_78568\\972751841.py:42: RuntimeWarning: coroutine 'process_file' was never awaited\n",
      "  process_file(file_path)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Main function to process the input file and generate a biography.\n",
    "async def process_file(file_path):\n",
    "    summarizer = Summarizer()\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        df = await read_csv(file_path)\n",
    "        if df is not None:\n",
    "            sprecher_prefix = 'IP_'\n",
    "            transkript_list = extract_rows_with_sprecher(df, sprecher_prefix)\n",
    "            transcript_data = transkript_to_string(transkript_list)\n",
    "        else:\n",
    "            print(\"Failed to read CSV file.\")\n",
    "            return\n",
    "    elif file_path.endswith('.docx'):\n",
    "        transcript_data = await read_docx(file_path)\n",
    "        if not transcript_data:\n",
    "            print(\"Failed to read DOCX file.\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Unsupported file format.\")\n",
    "        return\n",
    "\n",
    "    biography = await generate_biography(summarizer, transcript_data)\n",
    "    print(\"Biography : \\n \", count_words(biography), biography.strip())\n",
    "\n",
    "    x_biography = await final_biography(summarizer, biography)\n",
    "    summary = remove_incomplete_sentence(x_biography)\n",
    "    print(\"Summary : \\n \", count_words(summary), summary.strip())\n",
    "\n",
    "    if count_words(summary) <= 600:\n",
    "        refined_biography = summarizer.refine_biography(summary)\n",
    "        print(\"Refined : \\n\", count_words(refined_biography), refined_biography.strip())\n",
    "    else:\n",
    "        x = remove_incomplete_sentence(summary)\n",
    "        x1_biography = await final_2_biography(summarizer, x)\n",
    "        summary1 = remove_incomplete_sentence(x1_biography)\n",
    "        print(\"Summary 1: \", count_words(summary1), summary1.strip())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/adg0001_er_2024_04_23.csv\"\n",
    "    #file_path = \"C:/Users/asha4/OneDrive - SRH/Case Study-1/Dennis- Files/WG_ [EXTERN]  Transcripts and Biographies/adg0002_er_2024_04_23.csv\"\n",
    "    process_file(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
